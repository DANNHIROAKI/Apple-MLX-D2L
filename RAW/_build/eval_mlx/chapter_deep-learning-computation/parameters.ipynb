{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdee6fe6",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 参数管理\n",
    "\n",
    "在选择了架构并设置了超参数后，我们就进入了训练阶段。\n",
    "此时，我们的目标是找到使损失函数最小化的模型参数值。\n",
    "经过训练后，我们将需要使用这些参数来做出未来的预测。\n",
    "此外，有时我们希望提取参数，以便在其他环境中复用它们，\n",
    "将模型保存下来，以便它可以在其他软件中执行，\n",
    "或者为了获得科学的理解而进行检查。\n",
    "\n",
    "之前的介绍中，我们只依靠深度学习框架来完成训练的工作，\n",
    "而忽略了操作参数的具体细节。\n",
    "本节，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化；\n",
    "* 参数初始化；\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先看一下具有单隐藏层的多层感知机。**)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575fceea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.074915Z",
     "iopub.status.busy": "2024-07-24T07:43:03.074357Z",
     "iopub.status.idle": "2024-07-24T07:43:03.198163Z",
     "shell.execute_reply": "2024-07-24T07:43:03.197860Z"
    },
    "origin_pos": 5,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "X = mx.random.uniform(shape=(2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b04e89",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。\n",
    "当通过`Sequential`类定义模型时，\n",
    "我们可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样，每层的参数都在其属性中。\n",
    "如下所示，我们可以检查第二个全连接层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7378be5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.216563Z",
     "iopub.status.busy": "2024-07-24T07:43:03.216409Z",
     "iopub.status.idle": "2024-07-24T07:43:03.232227Z",
     "shell.execute_reply": "2024-07-24T07:43:03.231956Z"
    },
    "origin_pos": 10,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': array([[-0.347928, 0.0240712, -0.230242, ..., 0.277493, 0.248133, -0.215919]], dtype=float32),\n",
       " 'bias': array([0.306495], dtype=float32)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[2].parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75e2f1",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "输出的结果告诉我们一些重要的事情：\n",
    "首先，这个全连接层包含两个参数，分别是该层的权重和偏置。\n",
    "两者都存储为单精度浮点数（float32）。\n",
    "注意，参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数类的一个实例。\n",
    "要对参数执行任何操作，首先我们需要访问底层的数值。\n",
    "有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。\n",
    "下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，\n",
    "提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3da00dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.233708Z",
     "iopub.status.busy": "2024-07-24T07:43:03.233623Z",
     "iopub.status.idle": "2024-07-24T07:43:03.236153Z",
     "shell.execute_reply": "2024-07-24T07:43:03.235846Z"
    },
    "origin_pos": 16,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(mlx.core.array, array([0.306495], dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.layers[2].bias), net.layers[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e064500",
   "metadata": {
    "origin_pos": 20
   },
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。\n",
    "当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，\n",
    "因为我们需要递归整个树来提取每个子块的参数。\n",
    "下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc289154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.237645Z",
     "iopub.status.busy": "2024-07-24T07:43:03.237537Z",
     "iopub.status.idle": "2024-07-24T07:43:03.239918Z",
     "shell.execute_reply": "2024-07-24T07:43:03.239669Z"
    },
    "origin_pos": 24,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0 weight (8, 4)\n",
      "layers.0 bias (8,)\n",
      "('layers.0', 'weight', (8, 4)) ('layers.0', 'bias', (8,)) ('layers.2', 'weight', (1, 8)) ('layers.2', 'bias', (1,))\n"
     ]
    }
   ],
   "source": [
    "name, layers = net.named_modules()[::-1][0]\n",
    "for param in layers.parameters():\n",
    "    print(name, param, layers[param].shape)\n",
    "\n",
    "print(*[(name, param, layers[param].shape) for name, layers in net.named_modules()[1:][::-1] for param in layers.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea74989",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec484b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.241525Z",
     "iopub.status.busy": "2024-07-24T07:43:03.241409Z",
     "iopub.status.idle": "2024-07-24T07:43:03.243581Z",
     "shell.execute_reply": "2024-07-24T07:43:03.243305Z"
    },
    "origin_pos": 30,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.306495], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net['layers'][2]['bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9398649",
   "metadata": {
    "origin_pos": 31
   },
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。\n",
    "我们首先定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadf0339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.244919Z",
     "iopub.status.busy": "2024-07-24T07:43:03.244819Z",
     "iopub.status.idle": "2024-07-24T07:43:03.254330Z",
     "shell.execute_reply": "2024-07-24T07:43:03.254061Z"
    },
    "origin_pos": 36,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.479976],\n",
       "       [-0.479971]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                         nn.Linear(8, 4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = []\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.append(block1())\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(*block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f147970",
   "metadata": {
    "origin_pos": 37
   },
   "source": [
    "[**设计了网络后，我们看看它是如何工作的。**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1683931d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.255800Z",
     "iopub.status.busy": "2024-07-24T07:43:03.255686Z",
     "iopub.status.idle": "2024-07-24T07:43:03.257475Z",
     "shell.execute_reply": "2024-07-24T07:43:03.257210Z"
    },
    "origin_pos": 39,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (layers.0): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.1): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.2): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.3): Sequential(\n",
      "    (layers.0): Linear(input_dims=4, output_dims=8, bias=True)\n",
      "    (layers.1): ReLU()\n",
      "    (layers.2): Linear(input_dims=8, output_dims=4, bias=True)\n",
      "    (layers.3): ReLU()\n",
      "  )\n",
      "  (layers.4): Linear(input_dims=4, output_dims=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd2514",
   "metadata": {
    "origin_pos": 41
   },
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。\n",
    "下面，我们访问第一个主要的块中、第二个子块的第一层的偏置项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa88acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.258961Z",
     "iopub.status.busy": "2024-07-24T07:43:03.258834Z",
     "iopub.status.idle": "2024-07-24T07:43:03.261149Z",
     "shell.execute_reply": "2024-07-24T07:43:03.260889Z"
    },
    "origin_pos": 46,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.141924, 0.245702, -0.203424, ..., -0.473626, 0.289716, 0.353636], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet.layers[1].layers[0].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307da58b",
   "metadata": {
    "origin_pos": 47
   },
   "source": [
    "## 参数初始化\n",
    "\n",
    "知道了如何访问参数后，现在我们看看如何正确地初始化参数。\n",
    "我们在 :numref:`sec_numerical_stability`中讨论了良好初始化的必要性。\n",
    "深度学习框架提供默认随机初始化，\n",
    "也允许我们创建自定义初始化方法，\n",
    "满足我们通过其他规则实现初始化权重。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8655e",
   "metadata": {
    "origin_pos": 52,
    "tab": [
     "mlx"
    ]
   },
   "source": [
    "默认情况下，MLX会根据一个范围均匀地初始化权重和偏置矩阵，这个范围是根据输入和输出维度计算出的。MLX的nn.init模块提供了多种预置初始化方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3a098",
   "metadata": {
    "origin_pos": 53
   },
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。\n",
    "下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，\n",
    "且将偏置参数设置为0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f20dd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.262704Z",
     "iopub.status.busy": "2024-07-24T07:43:03.262599Z",
     "iopub.status.idle": "2024-07-24T07:43:03.297785Z",
     "shell.execute_reply": "2024-07-24T07:43:03.297488Z"
    },
    "origin_pos": 58,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00447087, -0.0227117, 0.016953, -0.00380947], dtype=float32),\n",
       " array(0, dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def init_normal(array):\n",
    "#     weight_fn = nn.init.normal(mean=0.0, std=0.01)\n",
    "#     bias_fn = nn.init.constant(0.0)\n",
    "#     if array.ndim > 1:\n",
    "#         array = weight_fn(array)\n",
    "#     else:\n",
    "#         array = bias_fn(array)\n",
    "#     return array\n",
    "\n",
    "# for module in net.modules():\n",
    "#     if isinstance(module, nn.Linear):\n",
    "#         module.update(mlx.utils.tree_map(lambda x: init_normal(x), module.parameters()))\n",
    "\n",
    "weight_fn = nn.init.normal(mean=0.0, std=0.01)\n",
    "bias_fn = nn.init.constant(0.0)\n",
    "for layer in net.layers:\n",
    "    if type(layer) == nn.Linear:\n",
    "        layer.weight = weight_fn(layer.weight)\n",
    "        layer.bias = bias_fn(layer.bias)\n",
    "\n",
    "net.layers[0].weight[0], net.layers[0].bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9e0ca",
   "metadata": {
    "origin_pos": 59
   },
   "source": [
    "我们还可以将所有参数初始化为给定的常数，比如初始化为1。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94132f5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.299503Z",
     "iopub.status.busy": "2024-07-24T07:43:03.299365Z",
     "iopub.status.idle": "2024-07-24T07:43:03.302939Z",
     "shell.execute_reply": "2024-07-24T07:43:03.302657Z"
    },
    "origin_pos": 64,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1], dtype=float32), array(0, dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def init_constant(array):\n",
    "#     weight_fn = nn.init.constant(1.0)\n",
    "#     bias_fn = nn.init.constant(0.0)\n",
    "#     if array.ndim > 1:\n",
    "#         array = weight_fn(array)\n",
    "#     else:\n",
    "#         array = bias_fn(array)\n",
    "#     return array\n",
    "\n",
    "# for module in net.modules():\n",
    "#     if isinstance(module, nn.Linear):\n",
    "#         module.update(mlx.utils.tree_map(lambda x: init_constant(x), module.parameters()))\n",
    "\n",
    "weight_fn = nn.init.constant(1.0)\n",
    "bias_fn = nn.init.constant(0.0)\n",
    "for layer in net.layers:\n",
    "    if type(layer) == nn.Linear:\n",
    "        layer.weight = weight_fn(layer.weight)\n",
    "        layer.bias = bias_fn(layer.bias)\n",
    "\n",
    "net.layers[0].weight[0], net.layers[0].bias[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2cc3c",
   "metadata": {
    "origin_pos": 65
   },
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。\n",
    "例如，下面我们使用Xavier初始化方法初始化第一个神经网络层，\n",
    "然后将第三个神经网络层初始化为常量值42。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1030c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.304419Z",
     "iopub.status.busy": "2024-07-24T07:43:03.304291Z",
     "iopub.status.idle": "2024-07-24T07:43:03.307491Z",
     "shell.execute_reply": "2024-07-24T07:43:03.307250Z"
    },
    "origin_pos": 70,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1], dtype=float32), array(0, dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_fn = nn.init.constant(1.0)\n",
    "bias_fn = nn.init.constant(0.0)\n",
    "for layer in net.layers:\n",
    "    if type(layer) == nn.Linear:\n",
    "        layer.weight = weight_fn(layer.weight)\n",
    "        layer.bias = bias_fn(layer.bias)\n",
    "\n",
    "net.layers[0].weight[0], net.layers[0].bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2e5980d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.308979Z",
     "iopub.status.busy": "2024-07-24T07:43:03.308873Z",
     "iopub.status.idle": "2024-07-24T07:43:03.311775Z",
     "shell.execute_reply": "2024-07-24T07:43:03.311541Z"
    },
    "origin_pos": 71,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([-0.238554, 0.592675, 0.250875, 0.421302], dtype=float32)\n",
      "array([[42, 42, 42, ..., 42, 42, 42]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# def init_xavier(array):\n",
    "#     if array.ndim > 1: # bias\n",
    "#         weight_fn = nn.init.glorot_uniform()\n",
    "#         array = weight_fn(array)\n",
    "#     return array\n",
    "\n",
    "# def init_42(array):\n",
    "#     if array.ndim > 1: # bias\n",
    "#         weight_fn = nn.init.constant(42.0)\n",
    "#         array = weight_fn(array)\n",
    "#     return array\n",
    "\n",
    "# for i, module in enumerate(net.modules()[1:][::-1]):\n",
    "#     if i == 0:\n",
    "#         module.update(mlx.utils.tree_map(lambda x: init_xavier(x), module.parameters()))\n",
    "#     if i == 2:\n",
    "#         module.update(mlx.utils.tree_map(lambda x: init_42(x), module.parameters()))\n",
    "\n",
    "uniform_fn = nn.init.glorot_uniform()\n",
    "const_fn = nn.init.constant(42.0)\n",
    "net.layers[0].weight = uniform_fn(net.layers[0].weight)\n",
    "net.layers[2].weight = const_fn(net.layers[2].weight)\n",
    "print(net.layers[0].weight[0])\n",
    "print(net.layers[2].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d277270",
   "metadata": {
    "origin_pos": 72
   },
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c3312b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.313072Z",
     "iopub.status.busy": "2024-07-24T07:43:03.312971Z",
     "iopub.status.idle": "2024-07-24T07:43:03.319146Z",
     "shell.execute_reply": "2024-07-24T07:43:03.318902Z"
    },
    "origin_pos": 81,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight (8, 4)\n",
      "Init weight (1, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.26438, 7.84126, 7.06211, -9.43468],\n",
       "       [0, 0, -9.56239, 9.42035]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, layer in net.named_modules()[::-1]:\n",
    "    if type(layer) == nn.Linear:\n",
    "        for param in layer.parameters():\n",
    "            print(\"Init\", param, layer[param].shape)\n",
    "            break\n",
    "        weight_fn = nn.init.uniform(low=-10, high=10)\n",
    "        layer.weight = weight_fn(layer.weight)\n",
    "        layer.weight *= mx.abs(layer.weight) >= 5\n",
    "\n",
    "net.layers[0].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0b3d3",
   "metadata": {
    "origin_pos": 82
   },
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a0156d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.320590Z",
     "iopub.status.busy": "2024-07-24T07:43:03.320488Z",
     "iopub.status.idle": "2024-07-24T07:43:03.322947Z",
     "shell.execute_reply": "2024-07-24T07:43:03.322724Z"
    },
    "origin_pos": 87,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 8.84126, 8.06211, -8.43468], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weight[:] += 1\n",
    "net.layers[0].weight[0, 0] = 42\n",
    "net.layers[0].weight[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b4343",
   "metadata": {
    "origin_pos": 89
   },
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数：\n",
    "我们可以定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "740ad932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T07:43:03.324338Z",
     "iopub.status.busy": "2024-07-24T07:43:03.324235Z",
     "iopub.status.idle": "2024-07-24T07:43:03.327323Z",
     "shell.execute_reply": "2024-07-24T07:43:03.327088Z"
    },
    "origin_pos": 94,
    "tab": [
     "mlx"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([True, True, True, ..., True, True, True], dtype=bool)\n",
      "array([True, True, True, ..., True, True, True], dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net.layers[2].weight[0] == net.layers[2].weight[0])\n",
    "net.layers[2].weight[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net.layers[2].weight[0]  == net.layers[4].weight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4886e312",
   "metadata": {
    "origin_pos": 97
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}